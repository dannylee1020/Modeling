{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora question pairs",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EPXGO57eLv_pRLJYqx-2CZYqgwbLH-0x",
      "authorship_tag": "ABX9TyOB92MnpkKWmJmJ2ov2EyvY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-nmikMHejVz",
        "colab_type": "text"
      },
      "source": [
        "# Objective\n",
        "We want to build a model that accurately predicts questions with same intent to reduce duplicated answer and confusion among the users. Accurately predicting duplicated questions and removing them will allow users to find high quality answers to questions resulting in improved experience for writers, seekers and readers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2Ve0JfDuI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a552d90b-7d8d-4cee-9147-e21f17a6e00f"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rc7hr2whthq",
        "colab_type": "text"
      },
      "source": [
        "# Prep Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO56BTnifgj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "5ea3c044-1547-400b-fe47-57ea00f78121"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rGXpRY4gYG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Data/quora_train.csv'\n",
        "train_data = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-e7M_v4ghpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4bebf497-f85c-4c40-d67c-319106c4c577"
      },
      "source": [
        "question_1 = train_data.iloc[:, 3]\n",
        "question_2 = train_data.iloc[:, 4]\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwZhAZOZQtmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create vocab list for validity purpose\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def clean_question(questions):\n",
        "  tokens = word_tokenize(questions)\n",
        "  tokens = [t for t in tokens if t.isalpha()]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [t for t in tokens if not t in stop_words]\n",
        "  tokens = [t for t in tokens if len(t) > 1]\n",
        "  return tokens\n",
        "\n",
        "def add_to_vocab(questions, vocab):\n",
        "  tokens = clean_question(questions)\n",
        "  vocab.update(tokens)\n",
        "\n",
        "def process_question(questions, vocab):\n",
        "  for q in questions:\n",
        "    q = str(q)\n",
        "    add_to_vocab(q, vocab)\n",
        "\n",
        "def save_list(tokens, filepath):\n",
        "  data = '\\n'.join(tokens)\n",
        "  file = open(filepath, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "question_1 = train_data.iloc[:, 3]\n",
        "question_2 = train_data.iloc[:, 4]\n",
        "questions = list(question_1) + list(question_2)\n",
        "vocab = Counter()\n",
        "process_question(questions, vocab)\n",
        "print(len(vocab))\n",
        "\n",
        "# remove low occurence words\n",
        "min_occurence = 2\n",
        "tokens = [k for k,c in vocab.items() if c > min_occurence]\n",
        "\n",
        "# save vocab\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "file_name = 'quora_question_vocab.txt'\n",
        "save_list(tokens, path+file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS-UTL0zAhwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzuK_FoBJFtN",
        "colab_type": "text"
      },
      "source": [
        "# Train Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuHDeXujJKvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create some functions to use\n",
        "\n",
        "def load_doc(filepath):\n",
        "  file = open(filepath,'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_question(question, vocab):\n",
        "  tokens = word_tokenize(question)\n",
        "  tokens = [w for w in tokens if w.isalpha()]\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "def process_question(question,vocab):\n",
        "  clean_q = list()\n",
        "  for q in question:\n",
        "    q = str(q)\n",
        "    qs = clean_question(q, vocab)\n",
        "    clean_q.append(qs)\n",
        "  return clean_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDMakNGMHeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load vocab\n",
        "vocab_filename = 'quora_question_vocab.txt'\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "vocab = load_doc(path+vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load traning dataset\n",
        "q1_cleaned = process_question(question_1, vocab)\n",
        "q2_cleaned = process_question(question_2, vocab)\n",
        "\n",
        "# traint test split\n",
        "VALIDATION_SPLIT = 0.4\n",
        "idx_split = round(len(q1_cleaned) * (1-VALIDATION_SPLIT))\n",
        "q1_split_train = q1_cleaned[:idx_split]\n",
        "q2_split_train = q2_cleaned[:idx_split]\n",
        "train_questions = q1_split_train + q2_split_train\n",
        "q1_split_test = q1_cleaned[idx_split:]\n",
        "q2_split_test = q2_cleaned[idx_split:]\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_questions)\n",
        "max_length = max([len(s.split()) for s in train_questions])\n",
        "\n",
        "# encode docs \n",
        "encoded_doc_q1 = tokenizer.texts_to_sequences(q1_split_train)\n",
        "encoded_doc_q2 = tokenizer.texts_to_sequences(q2_split_train)\n",
        "# pad sequences\n",
        "q1_train = pad_sequences(encoded_doc_q1, maxlen = max_length, padding = 'post')\n",
        "q2_train = pad_sequences(encoded_doc_q2, maxlen = max_length, padding = 'post')\n",
        "\n",
        "# test dataset\n",
        "encoded_q1_test = tokenizer.texts_to_sequences(q1_split_test)\n",
        "encoded_q2_test = tokenizer.texts_to_sequences(q2_split_test)\n",
        "q1_test = pad_sequences(encoded_q1_test, maxlen = max_length, padding = 'post')\n",
        "q2_test = pad_sequences(encoded_q2_test, maxlen = max_length, padding = 'post')\n",
        "\n",
        "# define target variable\n",
        "target = np.array(train_data['is_duplicate'])\n",
        "target_train = target[:idx_split]\n",
        "target_test = target[idx_split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQlldZ_y5pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4n2DBzyp0m",
        "colab_type": "text"
      },
      "source": [
        "# Build Model with Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8nyhgACzE8e",
        "colab_type": "text"
      },
      "source": [
        "## With Embedding and CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOXoQAutcXYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff2a56aa-37c8-4e6a-fa0a-3516eb7f316c"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D, Input, dot, Concatenate\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# # train test split for faster training and validation\n",
        "# perm = np.random.permutation(len(q1_train))\n",
        "# VALIDATION_SPLIT = 0.4\n",
        "# idx_train = perm[:int(len(q1_train)*(1-VALIDATION_SPLIT))] \n",
        "# idx_test = perm[int(len(q1_train)*(1-VALIDATION_SPLIT)):]\n",
        "\n",
        "# data_1_train = q1_train[idx_train]\n",
        "# data_2_train = q2_train[idx_train]\n",
        "# target_train = y_train[idx_train]\n",
        "\n",
        "# data_1_test = q1_train[idx_test]\n",
        "# data_2_test = q2_train[idx_test]\n",
        "# target_test = y_train[idx_test]\n",
        "\n",
        "# define vocab size \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Embedding\n",
        "q1_input = Input(shape=(max_length,))\n",
        "q1_embedding = Embedding(vocab_size, 150, input_length = max_length, )(q1_input)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(q1_embedding)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "q1_vector = Flatten()(x)\n",
        "\n",
        "q2_input = Input(shape = (max_length,))\n",
        "q2_embedding = Embedding(vocab_size, 150, input_length = max_length)(q2_input)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(q2_embedding)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(x)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "q2_vector = Flatten()(x)\n",
        "\n",
        "# connect to model\n",
        "prod = Concatenate(axis = 1)([q1_vector, q2_vector])\n",
        "# prod = dot([q1_vector, q2_vector], axes = 1) # cosine similarity\n",
        "x = Dense(128, activation = 'relu')(prod)\n",
        "x = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "model = Model(inputs = [q1_input, q2_input], outputs = x)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit([q1_train, q2_train], target_train, epochs = 10, verbose = 2, batch_size = 64,\n",
        "          validation_data = ([q1_test, q2_test], target_test))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 242574 samples, validate on 161716 samples\n",
            "Epoch 1/10\n",
            " - 60s - loss: 0.5125 - accuracy: 0.7488 - val_loss: 0.4793 - val_accuracy: 0.7699\n",
            "Epoch 2/10\n",
            " - 53s - loss: 0.3734 - accuracy: 0.8291 - val_loss: 0.4811 - val_accuracy: 0.7775\n",
            "Epoch 3/10\n",
            " - 53s - loss: 0.2290 - accuracy: 0.9015 - val_loss: 0.5667 - val_accuracy: 0.7705\n",
            "Epoch 4/10\n",
            " - 53s - loss: 0.1430 - accuracy: 0.9412 - val_loss: 0.7044 - val_accuracy: 0.7640\n",
            "Epoch 5/10\n",
            " - 53s - loss: 0.1011 - accuracy: 0.9600 - val_loss: 0.8792 - val_accuracy: 0.7622\n",
            "Epoch 6/10\n",
            " - 53s - loss: 0.0772 - accuracy: 0.9704 - val_loss: 0.9648 - val_accuracy: 0.7640\n",
            "Epoch 7/10\n",
            " - 53s - loss: 0.0604 - accuracy: 0.9766 - val_loss: 1.0670 - val_accuracy: 0.7674\n",
            "Epoch 8/10\n",
            " - 52s - loss: 0.0503 - accuracy: 0.9810 - val_loss: 1.1707 - val_accuracy: 0.7687\n",
            "Epoch 9/10\n",
            " - 52s - loss: 0.0431 - accuracy: 0.9839 - val_loss: 1.1986 - val_accuracy: 0.7697\n",
            "Epoch 10/10\n",
            " - 52s - loss: 0.0377 - accuracy: 0.9860 - val_loss: 1.3349 - val_accuracy: 0.7589\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 113)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 113)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 113, 150)     5250750     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 113, 150)     5250750     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 109, 128)     96128       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 109, 128)     96128       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 27, 128)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 27, 128)      0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 23, 128)      82048       max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 23, 128)      82048       max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 5, 128)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 5, 128)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 640)          0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 640)          0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1280)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          163968      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 11,021,949\n",
            "Trainable params: 11,021,949\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxxF39IfjfLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7meKMc4jf1k",
        "colab_type": "text"
      },
      "source": [
        "## With word2vec Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeOmQw8q7Y3Y",
        "colab_type": "text"
      },
      "source": [
        "### train and save embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLI-NTIj7mNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "f6838cb5-694f-42d6-a645-5e69155df809"
      },
      "source": [
        "# define some functions to create and save\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def load_doc(filepath):\n",
        "  file = open(filepath,'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_question(question, vocab):\n",
        "  q = str(question)\n",
        "  tokens = word_tokenize(q)\n",
        "  tokens = [w for w in tokens if w.isalpha()]\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return tokens\n",
        "\n",
        "def process_question(question,vocab):\n",
        "  clean_q = list()\n",
        "  for q in question:\n",
        "    q = str(q)\n",
        "    qs = clean_question(q, vocab)\n",
        "    clean_q.append(qs)\n",
        "  return clean_q\n",
        "\n",
        "# load vocab\n",
        "vocab_filename = 'quora_question_vocab.txt'\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "vocab = load_doc(path+vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# process training set\n",
        "question_1 = train_data.iloc[:, 3]\n",
        "question_2 = train_data.iloc[:, 4]\n",
        "q1_cleaned = process_question(question_1, vocab)\n",
        "q2_cleaned = process_question(question_2, vocab)\n",
        "sentences = q1_cleaned + q2_cleaned\n",
        "print(f''' Total Training Sentences: {len(sentences)}''')\n",
        "\n",
        "# train word2vec model\n",
        "model = Word2Vec(sentences, size = 100, window = 10, workers = 15, min_count = 1)\n",
        "words = list(model.wv.vocab)\n",
        "print(f'''Total number of words: {len(words)}''')\n",
        "\n",
        "# save model\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "filename = 'quora_w2v_embedding.txt'\n",
        "model.wv.save_word2vec_format(path+filename, binary = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Total Training Sentences: 808580\n",
            "Total number of words: 44818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuPjsED0XXxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk9cE17E7d-l",
        "colab_type": "text"
      },
      "source": [
        "### build model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHAfOvX98C5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdfbbbbe-b042-46c1-fa75-35fc8df850bf"
      },
      "source": [
        "# make some functions for model building\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "\n",
        "def load_doc(filepath):\n",
        "  file = open(filepath,'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_question(question, vocab):\n",
        "  question = str(question)\n",
        "  tokens = word_tokenize(question)\n",
        "  tokens = [w for w in tokens if w.isalpha()]\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "def process_question(question,vocab):\n",
        "  clean_q = []\n",
        "  for q in question:\n",
        "    q = str(q)\n",
        "    qs = clean_question(q, vocab)\n",
        "    clean_q.append(qs)\n",
        "  return clean_q\n",
        "\n",
        "def load_embedding(filepath):\n",
        "  file = open(filepath,'r')\n",
        "  lines = file.readlines()[1:]\n",
        "  file.close()\n",
        "  embeddings = {}\n",
        "  for line in lines:\n",
        "    parts = line.split()\n",
        "    word = parts[0]\n",
        "    vec = np.asarray(parts[1:], dtype = 'float32')\n",
        "    embeddings[word] = vec\n",
        "  return embeddings\n",
        "\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "  vocab_size = len(vocab) + 1\n",
        "  weight_matrix = np.zeros((vocab_size, 100))\n",
        "  for word, c in vocab.items():\n",
        "    weight_matrix[c] = embedding.get(word)\n",
        "  return weight_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qd7Si5qcC6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "7c8e9635-ade3-4650-c151-d2b53d82bfc8"
      },
      "source": [
        "temp = question_1[:20]\n",
        "process_question(temp, vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What step step guide invest share market india',\n",
              " 'What story Kohinoor Diamond',\n",
              " 'How increase speed internet connection using VPN',\n",
              " 'Why mentally lonely How solve',\n",
              " 'Which one dissolve water quikly sugar salt methane carbon di oxide',\n",
              " 'Astrology Capricorn Sun Cap moon cap rising say',\n",
              " 'Should buy',\n",
              " 'How good geologist',\n",
              " 'When use instead',\n",
              " 'Motorola company Can hack Charter',\n",
              " 'Method find separation slits using',\n",
              " 'How read find YouTube comments',\n",
              " 'What make Physics easy learn',\n",
              " 'What first sexual experience like',\n",
              " 'What laws change status student visa green card US compare immigration laws Canada',\n",
              " 'What would Trump presidency mean current international master students visa',\n",
              " 'What manipulation mean',\n",
              " 'Why girls want friends guy reject',\n",
              " 'Why many Quora users posting questions readily answered Google',\n",
              " 'Which best digital marketing institution banglore']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I40MINQXnILO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load vocab\n",
        "vocab_filename = 'quora_question_vocab.txt'\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "vocab = load_doc(path+vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load training data\n",
        "question_1 = train_data.iloc[:, 3]\n",
        "question_2 = train_data.iloc[:, 4]\n",
        "q1_cleaned = process_question(question_1, vocab)\n",
        "q2_cleaned = process_question(question_2, vocab)\n",
        "\n",
        "# train test split\n",
        "VALIDATION_SPLIT = 0.4\n",
        "idx_split = round(len(q1_cleaned) * (1-VALIDATION_SPLIT))\n",
        "q1_split_train = q1_cleaned[:idx_split]\n",
        "q2_split_train = q2_cleaned[:idx_split]\n",
        "sentences = q1_split_train + q2_split_train\n",
        "q1_split_test = q1_cleaned[idx_split:]\n",
        "q2_split_test = q2_cleaned[idx_split:]\n",
        "\n",
        "y_train = np.array(train_data['is_duplicate'])\n",
        "target_train = y_train[:idx_split]\n",
        "target_test = y_train[idx_split:]\n",
        "\n",
        "# tokenize and sequence encode\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "encoded_q1 = tokenizer.texts_to_sequences(q1_split_train)\n",
        "encoded_q2 = tokenizer.texts_to_sequences(q2_split_train)\n",
        "\n",
        "# pad sequence\n",
        "max_length = max([len(s.split()) for s in sentences]) # split to count number of words in each list of list for max padding\n",
        "q1_train = pad_sequences(encoded_q1, maxlen = max_length, padding = 'post')\n",
        "q2_train = pad_sequences(encoded_q2, maxlen = max_length, padding = 'post')\n",
        "\n",
        "# do the same for test set\n",
        "encoded_q1_test = tokenizer.texts_to_sequences(q1_split_test)\n",
        "encoded_q2_test = tokenizer.texts_to_sequences(q2_split_test)\n",
        "q1_test = pad_sequences(encoded_q1_test, maxlen = max_length, padding = 'post')\n",
        "q2_test = pad_sequences(encoded_q2_test, maxlen = max_length, padding = 'post')\n",
        "\n",
        "# define vocab size \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load and define embedding layer\n",
        "embedding_name = 'quora_w2v_embedding.txt'\n",
        "raw_embedding = load_embedding(path+embedding_name)\n",
        "embedding_vector = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "embedding_layer = Embedding(vocab_size, 100, input_length = max_length, weights = [embedding_vector],\n",
        "                            trainable =  False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc81BWtu714U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ab1ca65-d87e-46bd-fecd-f0a0eae9040f"
      },
      "source": [
        "print(q1_train.shape, q2_train.shape, q1_test.shape, q2_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(242574, 113) (242574, 113) (161716, 113) (161716, 113)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhjfXK_T78Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fef213c9-79bc-4293-f02a-5185d972afde"
      },
      "source": [
        "print(target_train.shape, target_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(242574,) (161716,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsRvMZ2W_pWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "de0271fa-7625-428e-9135-b17d229c8ed3"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Input, dot, Concatenate\n",
        "import keras.backend as K\n",
        "\n",
        "# build model\n",
        "K.clear_session()\n",
        "\n",
        "q1_input = Input(shape = (max_length,))\n",
        "q1_embedding = embedding_layer(q1_input)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(q1_embedding)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "q1_vector = Flatten()(x)\n",
        "\n",
        "q2_input = Input(shape = (max_length,))\n",
        "q2_embedding = embedding_layer(q2_input)\n",
        "x = Conv1D(filters = 128, kernel_size = 5, activation = 'relu')(q2_embedding)\n",
        "x = MaxPooling1D(pool_size = 4)(x)\n",
        "q2_vector = Flatten()(x)\n",
        "\n",
        "merged = Concatenate(axis = 1)([q1_input, q2_input]) # embedding already pre trained\n",
        "prod = Dense(128, activation = 'relu')(merged)\n",
        "prod = Dense(1, activation = 'sigmoid')(prod)\n",
        "\n",
        "model = Model(inputs = [q1_input, q2_input], outputs = prod)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit([q1_train, q2_train], target_train, epochs = 10, verbose = 2, batch_size = 64,\n",
        "          validation_data = ([q1_test, q2_test], target_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 242574 samples, validate on 161716 samples\n",
            "Epoch 1/10\n",
            " - 13s - loss: 18.1667 - accuracy: 0.5966 - val_loss: 5.2069 - val_accuracy: 0.6137\n",
            "Epoch 2/10\n",
            " - 13s - loss: 2.9318 - accuracy: 0.6099 - val_loss: 1.4218 - val_accuracy: 0.6355\n",
            "Epoch 3/10\n",
            " - 13s - loss: 0.9006 - accuracy: 0.6178 - val_loss: 0.7179 - val_accuracy: 0.6724\n",
            "Epoch 4/10\n",
            " - 13s - loss: 0.6385 - accuracy: 0.6507 - val_loss: 0.6249 - val_accuracy: 0.6696\n",
            "Epoch 5/10\n",
            " - 13s - loss: 0.6210 - accuracy: 0.6572 - val_loss: 0.6192 - val_accuracy: 0.6613\n",
            "Epoch 6/10\n",
            " - 13s - loss: 0.6202 - accuracy: 0.6546 - val_loss: 0.6178 - val_accuracy: 0.6650\n",
            "Epoch 7/10\n",
            " - 13s - loss: 0.6200 - accuracy: 0.6529 - val_loss: 0.6230 - val_accuracy: 0.6358\n",
            "Epoch 8/10\n",
            " - 13s - loss: 0.6177 - accuracy: 0.6528 - val_loss: 0.6188 - val_accuracy: 0.6666\n",
            "Epoch 9/10\n",
            " - 13s - loss: 0.6187 - accuracy: 0.6577 - val_loss: 0.6178 - val_accuracy: 0.6612\n",
            "Epoch 10/10\n",
            " - 14s - loss: 0.6179 - accuracy: 0.6568 - val_loss: 0.6224 - val_accuracy: 0.6621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f4002b88eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3CUYIviwzPK",
        "colab_type": "text"
      },
      "source": [
        "# Build Model With Gradient Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC9pET30w4BF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean questions\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def load_doc(filepath):\n",
        "  file = open(filepath, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_question(question, vocab):\n",
        "  tokens = word_tokenize(question)\n",
        "  tokens = [t for t in tokens if t.isalpha()]\n",
        "  tokens = [t for t in tokens if t in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "def process_question(question, vocab):\n",
        "  clean_q = []\n",
        "  for q in question:\n",
        "    qs = str(q)\n",
        "    qs = clean_question(qs, vocab)\n",
        "    clean_q.append(qs)\n",
        "  return clean_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmHRybAnDltX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load vocab\n",
        "vocab_filename = 'quora_question_vocab.txt'\n",
        "path = '/content/drive/My Drive/Data/'\n",
        "vocab = load_doc(path+vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# process question\n",
        "q1_cleaned = process_question(question_1, vocab)\n",
        "q2_cleaned = process_question(question_2, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIDpfsHclgR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "69b79f90-1910-40f6-a5bc-b0236eb860ac"
      },
      "source": [
        "q1_cleaned[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What step step guide invest share market india',\n",
              " 'What story Kohinoor Diamond',\n",
              " 'How increase speed internet connection using VPN',\n",
              " 'Why mentally lonely How solve',\n",
              " 'Which one dissolve water quikly sugar salt methane carbon di oxide',\n",
              " 'Astrology Capricorn Sun Cap moon cap rising say',\n",
              " 'Should buy',\n",
              " 'How good geologist',\n",
              " 'When use instead',\n",
              " 'Motorola company Can hack Charter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwsXlcxXE4o0",
        "colab_type": "text"
      },
      "source": [
        "## Bag of words + XGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGCmJNHlZCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import scipy as sp\n",
        "\n",
        "c_vect = CountVectorizer()\n",
        "c_vect.fit(q1_cleaned + q2_cleaned)\n",
        "train_q1_trans = c_vect.transform(q1_cleaned)\n",
        "train_q2_trans = c_vect.transform(q2_cleaned)\n",
        "\n",
        "X = sp.sparse.hstack((train_q1_trans, train_q2_trans))\n",
        "y = train_data['is_duplicate'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jslxrgAmUoJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "484c5975-32fb-4787-ba46-4abe1fed9d0d"
      },
      "source": [
        "import hyperopt\n",
        "from hyperopt import fmin, STATUS_OK, Trials, hp, tpe\n",
        "\n",
        "# Hyperparameter tuning\n",
        "def objective(space):\n",
        "  model = XGBClassifier(learning_rate = space['learning_rate'], colsample_bytree = space['colsample_bytree'], max_depth = space['max_depth'],\n",
        "                        gamma = space['gamma'], subsample = space['subsample'], n_estimators = space['n_estimators'], objective = 'binary:logistic')\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  score = f1_score(y_test, y_pred)\n",
        "  return {'loss': -score , 'status': STATUS_OK}\n",
        "\n",
        "# define search space\n",
        "def optimize(evals, trials, algo, random_state = 42):\n",
        "  space = {\n",
        "      'learning_rate': hp.quniform('learning_rate',0.01, 0.1, 0.01),\n",
        "      'colsample_bytree': hp.quniform('colsample_bytree',0.5, 1, 0.1),\n",
        "      'max_depth': hp.choice('max_depth', np.arange(30, 70, 10, dtype = int)),\n",
        "      'gamma': hp.uniform('gamma',0,3),\n",
        "      'subsample': hp.quniform('subsample',0.3, 1, 0.1),\n",
        "      'n_estimators': hp.choice('n_estimators', np.arange(80, 150, 10, dtype = int)),\n",
        "      'objective' : 'binary:logistic'\n",
        "  }\n",
        "  best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = evals, trials = trials)\n",
        "  return best\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "best_params = optimize(evals = 10,\n",
        "                       trials = trials,\n",
        "                        algo = tpe.suggest)\n",
        "print(best_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [1:08:34<00:00, 411.41s/it, best loss: -0.6155718501840752]\n",
            "{'colsample_bytree': 1.0, 'gamma': 2.4475221224301578, 'learning_rate': 0.06, 'max_depth': 3, 'n_estimators': 6, 'subsample': 0.6000000000000001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av1gu7hmHq02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "ad65811c-a5fc-4902-f2fa-aea949efb230"
      },
      "source": [
        "model = XGBClassifier(colsample_bytree = best_params['colsample_bytree'], gamma = best_params['gamma'], learning_rate = best_params['learning_rate'], \n",
        "                       max_depth = 50, n_estimators = 80, subsample = best_params['subsample'])\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('training score:', accuracy_score(y_train, model.predict(X_train)))\n",
        "print('test score:', accuracy_score(y_test, model.predict(X_test)))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training score: 0.7802649913016234\n",
            "test score: 0.7525971456132974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.93      0.83    101973\n",
            "           1       0.79      0.45      0.58     59743\n",
            "\n",
            "    accuracy                           0.75    161716\n",
            "   macro avg       0.77      0.69      0.70    161716\n",
            "weighted avg       0.76      0.75      0.73    161716\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB1DadJrQ40c",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF + LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyO-0MBM7kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_vect = TfidfVectorizer()\n",
        "t_vect.fit(q1_cleaned+q2_cleaned)\n",
        "q1_trans = t_vect.transform(q1_cleaned)\n",
        "q2_trans = t_vect.transform(q2_cleaned)\n",
        "\n",
        "X = sp.sparse.hstack((q1_trans, q2_trans))\n",
        "y = train_data['is_duplicate'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDjxkTkXM-ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fe226de4-dc62-4186-cf3a-14a353ee4860"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "def objective(space):\n",
        "  model = XGBClassifier(learning_rate = space['learning_rate'], colsample_bytree = space['colsample_bytree'], max_depth = space['max_depth'],\n",
        "                        gamma = space['gamma'], subsample = space['subsample'], n_estimators = space['n_estimators'], objective = 'binary:logistic')\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  score = f1_score(y_test, y_pred)\n",
        "  return {'loss': -score , 'status': STATUS_OK}\n",
        "\n",
        "# define search space\n",
        "def optimize(evals, trials, algo, random_state = 42):\n",
        "  space = {\n",
        "      'learning_rate': hp.quniform('learning_rate',0.01, 0.1, 0.01),\n",
        "      'colsample_bytree': hp.quniform('colsample_bytree',0.5, 1, 0.1),\n",
        "      'max_depth': hp.choice('max_depth', np.arange(30, 70, 10, dtype = int)),\n",
        "      'gamma': hp.uniform('gamma',0,3),\n",
        "      'subsample': hp.quniform('subsample',0.3, 1, 0.1),\n",
        "      'n_estimators': hp.choice('n_estimators', np.arange(80, 150, 10, dtype = int)),\n",
        "      'objective' : 'binary:logistic'\n",
        "  }\n",
        "  best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = evals, trials = trials)\n",
        "  return best\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "best_params = optimize(evals = 10,\n",
        "                       trials = trials,\n",
        "                        algo = tpe.suggest)\n",
        "print(best_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [1:29:22<00:00, 536.22s/it, best loss: -0.6453779209209896]\n",
            "{'colsample_bytree': 0.8, 'gamma': 1.2937410154165065, 'learning_rate': 0.07, 'max_depth': 1, 'n_estimators': 2, 'subsample': 0.9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnfSlpaNi3rd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "69a748e4-ae25-4cdc-c4c7-f52d68787686"
      },
      "source": [
        "# model\n",
        "model = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, \n",
        "                      reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('training score:', f1_score(y_train, model.predict(X_train)))\n",
        "print('test score:', f1_score(y_test, model.predict(X_test)))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training score: 0.7339071712688809\n",
            "test score: 0.6433254074452006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.90      0.83    101941\n",
            "           1       0.76      0.56      0.64     59775\n",
            "\n",
            "    accuracy                           0.77    161716\n",
            "   macro avg       0.77      0.73      0.74    161716\n",
            "weighted avg       0.77      0.77      0.76    161716\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW--ElMnhUwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}